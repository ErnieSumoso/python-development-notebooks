{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2577cf82-46bd-42d0-b35b-1d38499790a8",
   "metadata": {},
   "source": [
    "## Ernie Sumoso Vicuna\n",
    "## ID: 881591\n",
    "\n",
    "1-Add the custom stopwords “NIL” and “JUNK” in spaCy and remove the stopwords in below text\n",
    "\n",
    "Input : text=\" Jonas was a JUNK great guy NIL Adam was evil NIL Martha JUNK was more of a fool \"\n",
    "\n",
    "Expected Output :'Jonas great guy Adam evil Martha fool'\n",
    "\n",
    "2- Perform stemming/ convert each token to it’s root form in the given text\n",
    "\n",
    "Input :\n",
    "\n",
    "text= \"Dancing is an art. Students should be taught dance as a subject in schools . I danced in many of my school function. Some people are always hesitating to dance.\"\n",
    "\n",
    "text= 'danc is an art . student should be taught danc as a subject in school . I danc in mani of my school function . some peopl are alway hesit to danc .'\n",
    "\n",
    "3- How to merge two tokens as one ?\n",
    "\n",
    "Q. Merge the first name and last name as single token in the given sentence\n",
    "\n",
    "Input: text=\"Robert Langdon is a famous character in various books and movies \"\n",
    "\n",
    "Desired Output:\n",
    "- Robert Langdon\n",
    "- is\n",
    "- a\n",
    "- famous\n",
    "- character\n",
    "- in\n",
    "- various\n",
    "- books\n",
    "- and\n",
    "- movies\n",
    "\n",
    "4. Write a Python program to search for literal strings within a string.\n",
    "\n",
    "Sample text : 'The quick brown fox jumps over the lazy dog.'\n",
    "\n",
    "Searched words : 'fox', 'dog', 'horse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1d0ee64-662e-4349-b429-f85a3c5e8ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd66ccb0-2428-4164-8737-b08b479cbe90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7a5df90-9ff8-4679-b113-04bd2ca94718",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jonas great guy Adam evil Martha fool\n"
     ]
    }
   ],
   "source": [
    "# Question 1 solution\n",
    "sample_text = \" Jonas was a JUNK great guy NIL Adam was evil NIL Martha JUNK was more of a fool \"\n",
    "\n",
    "def tokenize_with_custom_stopwords(language, text, custom_stopwords=[]):\n",
    "    for custom_stopword in custom_stopwords:\n",
    "        STOP_WORDS.add(custom_stopword)\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    new_text = \"\"\n",
    "    for word in tokenized_text:\n",
    "        if word not in STOP_WORDS:\n",
    "            new_text += word + \" \"\n",
    "    return new_text.rstrip()\n",
    "    \n",
    "print(tokenize_with_custom_stopwords('en', sample_text, [\"NIL\", \"JUNK\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a85fdeb-5736-4175-859d-3d765ff2dc94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'danc is an art . student should be taught danc as a subject in school . i danc in mani of my school function . some peopl are alway hesit to danc .'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2 solution\n",
    "from nltk.stem import PorterStemmer\n",
    "sample_text = \"Dancing is an art. Students should be taught dance as a subject in schools . I danced in many of my school function. Some people are always hesitating to dance.\"\n",
    "\n",
    "def stem_all_words(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    new_text = \"\"\n",
    "    porter = PorterStemmer()\n",
    "    for word in tokenized_text:\n",
    "        stem_word = porter.stem(word)\n",
    "        new_text += stem_word + \" \"\n",
    "    return new_text.rstrip()\n",
    "\n",
    "stem_all_words(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9e33048e-8acf-4622-9427-c21e379299ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'what',\n",
       " 'is',\n",
       " 'this',\n",
       " 'Robert Langdon Sumoso',\n",
       " 'is',\n",
       " 'a',\n",
       " 'famous',\n",
       " 'character',\n",
       " 'like',\n",
       " 'Jimmy Neutron',\n",
       " 'in',\n",
       " 'various',\n",
       " 'books',\n",
       " 'and',\n",
       " 'movies']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3 solution\n",
    "sample_text = \"Hello what is this Robert Langdon Sumoso is a famous character like Jimmy Neutron in various books and movies \"\n",
    "def tokenize_names(text):\n",
    "    ttext = word_tokenize(text)\n",
    "    indexes = []\n",
    "    # check capital words and save indexes\n",
    "    for i in range(len(ttext)):\n",
    "        if ttext[i][0].isupper():\n",
    "            indexes.append(i)\n",
    "    if len(indexes) <= 1: return ttext\n",
    "\n",
    "    # glue all capital words that are together, save indexes to delete\n",
    "    start_i = indexes[0]\n",
    "    prev_i = indexes[0]\n",
    "    indexes_delete = []\n",
    "    for i in indexes[1:]:\n",
    "        if prev_i + 1 == i:\n",
    "            ttext[start_i] += ' ' + ttext[i]\n",
    "            indexes_delete.append(i)\n",
    "        else:\n",
    "            start_i = i\n",
    "        prev_i = i\n",
    "    \n",
    "    # delete remaining indexes (last names)\n",
    "    for count, i in enumerate(indexes_delete):\n",
    "        del ttext[i - count]\n",
    "    \n",
    "    return ttext\n",
    "tokenize_names(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea548226-4b21-47a5-8962-8b651d592a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fox', 'dog']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 4 solution\n",
    "import re\n",
    "# Write a Python program to search for literal strings within a string.\n",
    "# Sample text : 'The quick brown fox jumps over the lazy dog.'\n",
    "\n",
    "# Searched words : 'fox', 'dog', 'horse'\n",
    "sample = 'The quick brown fox jumps over the lazy dog.'\n",
    "search = ['fox', 'dog', 'horse']\n",
    "\n",
    "def search_literal_strings(text, search_words):\n",
    "    found = []\n",
    "    for search_word in search_words:\n",
    "        reg = r\"\\b\" + re.escape(search_word) + r\"\\b\"\n",
    "        found += re.findall(reg, text)\n",
    "    return found\n",
    "\n",
    "search_literal_strings(sample, search)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
